# Deep Generative Models in Robotics: A Survey on Learning from Multimodal Demonstrations

[Deep Generative Models in Robotics: A Survey on Learning from Multimodal
Demonstrations](https://arxiv.org/pdf/2408.04380)

# **Questions:**

- What is “supervision”? How can it be online / offline? *The survey focuses mainly on approaches that consider offline data, i.e., no additional data collected online or interactively, and offline supervision, i.e., no additional supervision other than expert actions.*

# **Behavior Cloning definition**

The central idea is to ensure that the samples generated by the model  a „ ρθpa|cq are indistinguishable from the real data samples a „ ρDpa|cq:.

$$
a \sim ρ_θ(a|c) <=> a \sim ρ_D(a|c)
$$

The task of learning the generative model is then formulated as an optimization problem, where the objective is to minimize the divergence between the learned distribution ρθpa|cq and the true data
distribution ρDpa|cq:

$$
\theta^* = arg \ min_\theta \ \mathbb{E}_{a,c \sim D}[\mathbb{D}(ρ_D(a|c), ρ_\theta(a|c))]
$$

where, **D** is the divergence distance

# **Models**

- **Sampling Models**
    - Given a noise sample, these models generate the action directly.
    - Variational Auto Encoders, Generative Adversarial Networks, Normalizing Flows
    
    ![{992D7968-DB24-481B-B15E-06516C3308DD}.png](Deep%20Generative%20Models%20in%20Robotics%20A%20Survey%20on%20Lea%207486e6a1aad9449cb1a6ae671bbd1ebb/992D7968-DB24-481B-B15E-06516C3308DD.png)
    
     
    
- **Energy-based Models**
    - Given an action candidate as input, EBM returns a scalar value representing the energy of that action candidate.
    
    ![{E8F77AD3-FBA0-4C94-B2A9-36866E141EDC}.png](Deep%20Generative%20Models%20in%20Robotics%20A%20Survey%20on%20Lea%207486e6a1aad9449cb1a6ae671bbd1ebb/E8F77AD3-FBA0-4C94-B2A9-36866E141EDC.png)
    
    - **Neural Descriptor Fields**: EBM as the Euclidean distance to a target action. These methods propose learning a feature encoder ϕθ, which computes a latent vector for a given input. The feature encoder is conditioned on the pointcloud of the object ϕθpa, cq, where c is the pointcloud. In [“Clip-fields: Weakly supervised semantic fields for robotic memory”], the CLIP [108] features are used to learn the feature encoder.
- **Diffusion Models**
    - generative model that learns to generate data by reversing a gradual corruption
    process. Frame the data generation as an iterative denoising process.
    
    ![{922767F5-0DAA-44DC-A462-D341272046A2}.png](Deep%20Generative%20Models%20in%20Robotics%20A%20Survey%20on%20Lea%207486e6a1aad9449cb1a6ae671bbd1ebb/922767F5-0DAA-44DC-A462-D341272046A2.png)
    
    - Equation definition
    - Consistency Policies [113], explores how to make DM sampling faster.
    - Classifier Guidance [118] proposes to combine the output of an unconditional DM with
    the gradient of a classifier in the generation process.
    - Classifier-free guidance [119] proposes instead to combine an unconditional DM with a conditioned DM. In [111], [120], several conditioned DM were combined for modular generation.
- **Categorical Models**
    - We refer to Categorical Models as the set of generative models that, given a context variable c as input, output the probability of K different categories, where K is finite.
        - output K logit ⇒ Softmax converts the logits into probability values for each action
    - **Action Value Maps** *given a visual observation o as input, output a value map H of the same form as the input, H “ Qθpoq, similar to attention maps*
    
    ![{12C7A9C0-9D2A-4607-B688-EA97E3F3DBE3}.png](Deep%20Generative%20Models%20in%20Robotics%20A%20Survey%20on%20Lea%207486e6a1aad9449cb1a6ae671bbd1ebb/12C7A9C0-9D2A-4607-B688-EA97E3F3DBE3.png)
    
    - **Autoregressive Models**: Autoregressive models are a set of generative models that
    generate long-horizon data by iteratively invoking the model while conditioning on past data.
        - In certain cases, the action distribution is represented by DM [141], [142]
        - or by adapting the means of the categorical distribution [59], [143], [144].
    
    ![{19593E7C-BBB5-4D53-B9CC-4DCDF99C1BA0}.png](Deep%20Generative%20Models%20in%20Robotics%20A%20Survey%20on%20Lea%207486e6a1aad9449cb1a6ae671bbd1ebb/19593E7C-BBB5-4D53-B9CC-4DCDF99C1BA0.png)
    
- **Mixture Density Models**
    - Given a context variable, Mixture Density Models (MDM) returns the parameters of a mixture density function representing the action distribution.
    
    ![{788F15BC-496F-44B6-873C-F99E15957977}.png](Deep%20Generative%20Models%20in%20Robotics%20A%20Survey%20on%20Lea%207486e6a1aad9449cb1a6ae671bbd1ebb/788F15BC-496F-44B6-873C-F99E15957977.png)
    

# Generalizing Outside Data Distributions

- **Composition**
    
    *Rather than learning a monolithic policy, several researchers have explored learning individual behavior modules that can later be composed to generate complex behaviors. This composition can be both parallel (combining the behaviors together) and sequential (for generating long horizon tasks).*
    
- **Feature Selection from Observations**
    
    *reducing the observations to informative features, thus improving the system’s ability to learn meaningful correlations.*
    
    - Generalization is facilitated by learning an encoder z from E(c) that is capable of producing latent representations z that capture the relevant features to solve the robotics tasks. A common approach is to extract some form of object-centric features from the images, usually related to the location of the objects.
        - pose estimation
        - set of key points
        - extract a set of cropped images through bounding boxes
        - segmentation masks
        - language conditioned semantic features → *exploit the pre-trained vision language
        models efficiently*
        - tactile information
    
- **Observation-Action Symmetries**
    
    *research explores the problem of representing both visual observations and actions in a common space*
    

### **Imitation Learning challenges**

- Demonstration diversity: tradeoff between unimodal distribution and different skill-level experts.
- Heterogeneous Action and State Spaces.
- Partially observable Demonstrations: solution → encode history of observations as context.

## **Future Research**

- Synthetic data ok if sim-to-real-gap.
- Online exploration of the model.
- Generalization: not shown yet, *internet knowledge good.*
- 3D feature fields → Act3D, Distilled feature fields enable few-shot language-guided manipulation.