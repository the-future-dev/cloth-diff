# Experiment 0

## Experiments

1. **Reproduction of Diffusion Policy Results**

with CNN-Based Diffusion Policy

- keypoints: 88%
- image: 70%

https://wandb.ai/fiatlux/diffusion-pusht-keypoints?nw=nwuserandrearitossa

**PushT - Exploiting privileged Information:**

1. Shared Encoder with Keypoints at 0 during inference

Result: not an improved learning curve: it is the same percentage of success vs steps trajectory as image only

![**Blue**: image only training | **Light** **Green**: Image+Keypoints as privileged input | **Dark green**: image and keypoints at training and inference (expert)](Experiment%200%201ac7ec91f05a802c8168eb450795cf21/image.png)

**Blue**: image only training | **Light** **Green**: Image+Keypoints as privileged input | **Dark green**: image and keypoints at training and inference (expert)

1. 

1. **Testing different privileged information exploitation recipes**
- **IDEAS from LUPI:**
1. choose weights from teacher ⇒ enforce learning of the Teacher Neural Network Extrapolated Features.
    1.  multi-task learning (MTL) model with a shared encoder:
        
        **Architecture design:**
        
        **Shared Encoder**: A neural network (e.g., a convolutional neural network for images or a fully connected network for tabular data) processes the basic inputs to produce a feature encoding. This encoder learns representations based on the basic inputs alone.
        
        **Two Output Heads**:
        
        - **Main Task Head**: Takes the encoded features and predicts the primary output (e.g., classification or regression). This head is used during both training and inference.
        - **Privileged Prediction Head**: Takes the same encoded features and predicts the privileged inputs (or some function of them). This auxiliary task is active only during training, using the privileged data as targets.
        
        **Loss Function**: During training, the total loss is a weighted combination of the main task loss (e.g., cross-entropy) and the privileged prediction loss (e.g., mean squared error if reconstructing privileged features). For example:
        
        $$
        \text{Loss} = \text{Loss}_{\text{main}} + \lambda \cdot \text{Loss}_{\text{privileged}} 
        $$
        
        where λ balances the two objectives.
        
    
2. weight example importance based on difficulty; if too difficult: weight to zero. In case of Neural Network adjust the learning rate
    - interesting because of trembling in the end of pushing: weight based on how fast we go to goal state
- **IDEAS from Multimodal Learning**
1. **Modality Dropout**:
    - In multimodal learning, models like those for vision-and-language tasks use **modality dropout** to handle missing modalities. During training, you randomly drop one modality (e.g., text) with some probability, forcing the model to rely on the remaining one (e.g., images).
    - **Application**: Treat your privileged inputs as a separate modality. During training, randomly drop the privileged inputs for some samples, forcing the shared encoder to produce robust features from basic inputs alone. This ensures the model learns to function without privileged data, aligning with inference conditions.
    - **Implementation**: In the MTL architecture, occasionally set the privileged input to zero or skip the privileged prediction loss computation, training the encoder to adapt to both scenarios (with and without privileged data).
    1. **?? Contrastive Learning** 

- **Ideas from Reinforcement Learning**
    1. **Architecture**:
        - **Actor**: A policy network that takes basic inputs and outputs actions. It’s the primary component used during inference.
        
        The Actor network objective is to learn the distribution of behaviours to complete the task, from Behaviour Cloning.
        
        - **Critic**: A value network that estimates the expected reward (value function) for given states or state-action pairs. During training, it uses both basic and privileged inputs for more accurate estimates.
    
    After some time of Behaviour cloning (convergence?) The training paradigm should change. The idea is to evaluate the actions proposed from the model based on a Mathematical Ground Truth: the privileged information that compares the current target displacement against the goal displacement.
    
    Here we aim to incentivize task completion: Citing from paper “Meta Reinforcement Learning Fine-tuning”.
    
    Te RL paradigm: maximize a bonus reward (in conjunction with the BC) that represent the progress made in the completion of the task, quantified in the change of likelihood of eventual success
    
    - more practically, the privileged information grants us free access to a mathematical formulation of task success.
    
    Example of PushT: the objective of the task is to change the displacement of the target T (as in all manipulation tasks and DOM objects). The reward applied to the action should be directly proportional to the progress that it provides toward the minimization of the difference between the T keypoints and the goal keypoints, both from a rotational and a transational poit of view.
    
    The other way around: a loss should be applied on the .
    
    Furthermore, following LUPI, access to privileged information of such a type durimg training allows to the definition of an auxiliary loss while executing behaviour cloning itself. Evaluating the action impact towards the goal is feasible at training time, options to incorporating it are two:
    
    - an auxiliary loss
    - or by weigting the learning rate proportionally to the progress made in the action
    
    This weighting should be wise not to disincentivize learning how to recover from unexpected situations but it’s main purpose is to prevent the unexpected situation by default: a behaviour often encountered in BC due to the imperfection of human simulations.
    
    Example: in pushT, 2D, 8 points (x,y).
    
    the progress is simply made by evaluating the progress done for rotational and transational displacement.
    
    The rigidity of the object allows to achieve it with two variables: rot, and trans.
    
    To expand the framework and make it flexible for applying it, to deformables, we can compute rotation and translation for each point (mesh). ((no rotation for a single point)).
    
    - Based on task: is the reward based on absolute or relative position? Relative. Evaluate at test time.
    - Absolute: ok→ difference between current point position (3d)
    - relative: Normalizes for different positions in the space: good when folding a T-shirt
    
    Real World - Gap
    
    What is the current state? State Estimation ex: cloth-splatting
    
    / object segmentation.
    
    - How well can cloth splatting generalize? Can I use it for Real PushT? Can i do it efficiently? (nobody wants to re-train…)
    
    Learn a task-based distribution of terminal outcomes. And average through it. Allows for ** ONLINE ** after the first human demonstration.
    
    - idea is to store human goal state, from demonstration (RL: 1 as reward, end-state). Lightweight learn a distribution / just average it.
    
    ![Screenshot_2025-03-12-04-46-41-904_com.android.chrome.jpg](Experiment%200%201ac7ec91f05a802c8168eb450795cf21/Screenshot_2025-03-12-04-46-41-904_com.android.chrome.jpg)
    
    At inference:
    
    - non trajectory based but based on a single action step
    - Ground Truth State (the Privileged Info) provides us a gratis reward function. Use Gradient multiplied by the reward: positive updates forward a correct behaviour, negative neutralizes the learning of a destructive behaviour.
    
    ![Screenshot_2025-03-12-06-05-55-887_com.android.chrome.jpg](Experiment%200%201ac7ec91f05a802c8168eb450795cf21/Screenshot_2025-03-12-06-05-55-887_com.android.chrome.jpg)
    
    solo idea:
    
    Domanda successiva. É possibile parametrizzare la reward function in base ad un dataset senza privileged actions?
    
    - Sí:
    - step 1 aggrego tutti i last frames di ogni episodio e computo la distribuzione degli end states (max reward).
    - step 2: stimo il reward con una network (suona molto male). Parto da 1 sull’ultimo frame, discount in base al processo per i reward degli episodi precedenti. (Missing: failure?)
    
    1. **?? Look at paper Optimized RL** 

![17417604720275174862819182615027.jpg](Experiment%200%201ac7ec91f05a802c8168eb450795cf21/17417604720275174862819182615027.jpg)

## Report Experiment_0

graphs

Sucess rate vs :

- number of demos
- … *hyperparameters such as*
- number of diffusion steps
- number of actions denoised
- type of noise scheduling
- [data collections (human, automation, optimals, (exploration)…., ]